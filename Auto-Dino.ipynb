{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Dino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing all the packages required to implement Auto-Dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, Nadam\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Path Variables as well as preparing the scripts to create id from DOM and getting the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path Variables\n",
    "GAME_URL = \"http://wayou.github.io/t-rex-runner/\"\n",
    "CHROME_DRIVER_PATH = \"./chromedriver\"\n",
    "LOSS_FILE_PATH = \"./objects/loss_df.csv\"\n",
    "ACTIONS_FILE_PATH = \"./objects/actions_df.csv\"\n",
    "Q_VALUE_FILE_PATH = \"./objects/q_values.csv\"\n",
    "SCORE_FILE_PATH = \"./objects/scores_df.csv\"\n",
    "\n",
    "# Script to create id for canvas for faster selections from Document Object Model (DOM)\n",
    "init_script = \"document.getElementsByClassName('runner-canvas')[0].id = 'runner-canvas'\"\n",
    "\n",
    "# Script to get image from canvas\n",
    "getbase64Script = \"canvasRunner = document.getElementById('runner-canvas'); \\\n",
    "return canvasRunner.toDataURL().substring(22)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaring all the game parameter constants and initializing the log structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game Parameter Constants\n",
    "ACTIONS = 2  # Possible actions: \"Jump\" or \"Do Nothing\"\n",
    "GAMMA = 0.9  # Decay rate of past observations, original 0.99\n",
    "OBSERVATION = 100.  # Timesteps to observe before training\n",
    "EXPLORE = 100000  # Frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001  # Final value of epsilon\n",
    "INITIAL_EPSILON = 0.1  # Initial value of epsilon\n",
    "REPLAY_MEMORY = 80000  # Number of previous transitions to remember\n",
    "BATCH = 32  # Size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 0.0003\n",
    "\n",
    "img_rows, img_cols = 80, 80\n",
    "img_channels = 4  # We stack 4 frames\n",
    "\n",
    "# Initialize log structures from file if they exist or else create new\n",
    "loss_df = pd.read_csv(LOSS_FILE_PATH) if os.path.isfile(\n",
    "    LOSS_FILE_PATH) else pd.DataFrame(columns=[\"loss\"])\n",
    "score_df = pd.read_csv(SCORE_FILE_PATH) if os.path.isfile(\n",
    "    SCORE_FILE_PATH) else pd.DataFrame(columns=[\"Scores\"])\n",
    "actions_df = pd.read_csv(ACTIONS_FILE_PATH) if os.path.isfile(\n",
    "    ACTIONS_FILE_PATH) else pd.DataFrame(columns=[\"Actions\"])\n",
    "q_values_df = pd.read_csv(Q_VALUE_FILE_PATH) if os.path.isfile(\n",
    "    Q_VALUE_FILE_PATH) else pd.DataFrame(columns=[\"qvalues\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions\n",
    "\n",
    "process_image : The raw image captured has a resolution of around 600x150 with 3 (RGB) channels. We intend to use 4 consecutive screenshot as a single input to the model. So we use the OpenCV library to resize, crop and process the image. The final processed input is of just 80x80 pixels and single channeled (grey scale).\n",
    "\n",
    "grab_screen : The game screen is captured using PIL by taking a screenshot of entire screen and croping the region of interest. A base64 formatted image is obtained using JavaScript with the help of selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic pre-processing function\n",
    "\n",
    "def save_object(object, name):\n",
    "    \"\"\"\n",
    "    Dump file into objects folder\n",
    "    \"\"\"\n",
    "    with open(\"objects/\" + name + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(object, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_object(name):\n",
    "    \"\"\"\n",
    "    Loads file Dump\n",
    "    \"\"\"\n",
    "    with open(\"objects/\" + name + \".pkl\", \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def process_image(image):\n",
    "    \"\"\"\n",
    "    Processes the image to use futher\n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # RGB to Gray scale\n",
    "    image = image[:300, :500]  # Crop Region of Interest(ROI)\n",
    "    image = cv2.resize(image, (80, 80))\n",
    "    return image\n",
    "\n",
    "\n",
    "def grab_screen(_driver):\n",
    "    \"\"\"\n",
    "    Grabs the screen\n",
    "    \"\"\"\n",
    "    image_b64 = _driver.execute_script(getbase64Script)\n",
    "    screen = np.array(Image.open(BytesIO(base64.b64decode(image_b64))))\n",
    "    image = process_image(screen)  # Processing image is required\n",
    "    return image\n",
    "\n",
    "\n",
    "def show_image(graphs=False):\n",
    "    \"\"\"\n",
    "    Shows images in new window\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"Logs\" if graphs else \"Game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)\n",
    "        image_size = cv2.resize(screen, (800, 400))\n",
    "        cv2.imshow(window_title, screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord(\"q\")):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the filesystem structure\n",
    "Done only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainig varialbes saved as checkpoints to filesystem to resume training from the same step\n",
    "\n",
    "def init_cache():\n",
    "    \"\"\"\n",
    "    Initiate variable caching. Done only once\n",
    "    \"\"\"\n",
    "    save_object(INITIAL_EPSILON, \"epsilon\")\n",
    "    t = 0\n",
    "    save_object(t, \"time\")\n",
    "    D = deque()\n",
    "    save_object(D, \"D\")\n",
    "\n",
    "init_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Module\n",
    "We implement the interfacing between Python and JavaScript using this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game():\n",
    "    \"\"\"\n",
    "    Selenium interfacing between the python and browser\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, custom_config=True):\n",
    "        \"\"\"\n",
    "        Launch the broswer window using the attributes in chrome_options\n",
    "        \"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        chrome_options.add_argument(\"--mute-audio\")\n",
    "        self._driver = webdriver.Chrome(\n",
    "            executable_path=CHROME_DRIVER_PATH, chrome_options=chrome_options)\n",
    "\n",
    "        self._driver.set_window_position(x=-10, y=0)\n",
    "        self._driver.get(\"chrome://dino\")\n",
    "        self._driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "        self._driver.execute_script(init_script)\n",
    "\n",
    "    def get_crashed(self):\n",
    "        \"\"\"\n",
    "        return True if the agent as crashed on an obstacles. Gets javascript variable from game decribing the state\n",
    "        \"\"\"\n",
    "        return self._driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "\n",
    "    def get_playing(self):\n",
    "        \"\"\"\n",
    "        returns True if game in progress, false is crashed or paused\n",
    "        \"\"\"\n",
    "        return self._driver.execute_script(\"return Runner.instance_.playing\")\n",
    "\n",
    "    def restart(self):\n",
    "        \"\"\"\n",
    "        Sends a signal to browser-javascript to restart the game\n",
    "        \"\"\"\n",
    "        self._driver.execute_script(\"Runner.instance_.restart()\")\n",
    "\n",
    "    def press_up(self):\n",
    "        \"\"\"\n",
    "        Sends a single to press up get to the browser\n",
    "        \"\"\"\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_UP)\n",
    "\n",
    "    def get_score(self):\n",
    "        \"\"\"\n",
    "        Gets current game score from javascript variables\n",
    "        \"\"\"\n",
    "        score_array = self._driver.execute_script(\n",
    "            \"return Runner.instance_.distanceMeter.digits\")\n",
    "        # the javascript object is of type array with score in the formate[1,0,0] which is 100.\n",
    "        score = ''.join(score_array)\n",
    "        return int(score)\n",
    "\n",
    "    def pause(self):\n",
    "        \"\"\"\n",
    "        Pause the game\n",
    "        \"\"\"\n",
    "        return self._driver.execute_script(\"return Runner.instance_.stop()\")\n",
    "\n",
    "    def resume(self):\n",
    "        \"\"\"\n",
    "        Resume a paused game if not crashed\n",
    "        \"\"\"\n",
    "        return self._driver.execute_script(\"return Runner.instance_.play()\")\n",
    "\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        Close the browser and end the game\n",
    "        \"\"\"\n",
    "        self._driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Module\n",
    "\n",
    "We wrap all the interfacing using Agent Module. We control the Dino using this module as well as get status of the agent in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoAgent:\n",
    "    \"\"\"\n",
    "    Reinforcement Agent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, game):  # takes game as input for taking actions\n",
    "        self._game = game\n",
    "        self.jump()  # to start the game, we need to jump once\n",
    "\n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "\n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n",
    "\n",
    "    def jump(self):\n",
    "        self._game.press_up()\n",
    "\n",
    "    def duck(self):\n",
    "        self._game.press_down()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game State Module\n",
    "\n",
    "To send actions to the module and get a resultant state that the environment transitions into as a result of that action, we use the Game_State module. It simplifies the process by receiving & performing actions, decide the reward and return the experience tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_State:\n",
    "    def __init__(self, agent, game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        # Display the processed image on screen using openCV, implemented using python coroutine\n",
    "        self._display = show_image()\n",
    "        self._display.__next__()  # Initilize the display coroutine\n",
    "\n",
    "    def get_state(self, actions):\n",
    "        \"\"\"\n",
    "        Returns the Experience of one itereationas a tuple\n",
    "        \"\"\"\n",
    "        actions_df.loc[len(actions_df)\n",
    "                       ] = actions[1]  # Storing actions in a dataframe\n",
    "        score = self._game.get_score()\n",
    "        reward = 0.1\n",
    "        is_over = False  # Game Over\n",
    "        if actions[1] == 1:\n",
    "            self._agent.jump()\n",
    "        image = grab_screen(self._game._driver)\n",
    "        self._display.send(image)  # Display the image on screen\n",
    "        if self._agent.is_crashed():\n",
    "            # Log the score when the game is over\n",
    "            score_df.loc[len(loss_df)] = score\n",
    "            self._game.restart()\n",
    "            reward = -1\n",
    "            is_over = True\n",
    "        return image, reward, is_over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "We use a series of three Convolution layers before flattening them to dense layers and output layer. Our output layers consists of two neurons, each representing the maximum predicted reward for each action. We then choose the action with maximum reward (Q-value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel():\n",
    "    print(\"Building Convolutional Neural Network\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), padding=\"same\", strides=(4, 4), input_shape=(\n",
    "        img_cols, img_rows, img_channels)))  # First layer of 80*80*4 with 32 filters\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    # Second layer of 40*40*4 with 64 filters\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    # Third layer of 30*30*4 with 64 filters\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    #adam = Adam(lr=LEARNING_RATE)\n",
    "    nadam = Nadam(lr=LEARNING_RATE)\n",
    "    model.compile(loss=\"mse\", optimizer=nadam)\n",
    "\n",
    "    # Creating model file if not present\n",
    "    if not os.path.isfile(LOSS_FILE_PATH):\n",
    "        model.save_weights(\"model.h5\")\n",
    "    print(\"Finished building the Convolutional Neural Network\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process\n",
    "\n",
    "Things that happen during the training process\n",
    "\n",
    "1. Start with no action and get initial state (s_t)\n",
    "2. Observe game-play for OBSERVATION number of steps\n",
    "3. Predict and perform an action\n",
    "4. Store experience in Replay Memory\n",
    "5. Choose a batch randomly from Replay Memory and train model on it\n",
    "6. Restart if game over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNetwork(model, game_state, observe=False):\n",
    "    \"\"\"\n",
    "    Main Training module\n",
    "\n",
    "    Parameters:\n",
    "        model => Keras Model to be trained\n",
    "        game_state => Game State module with access to game environment and dino\n",
    "        observe => Flag to indicate if the model is to be trained(weights updates), else just play\n",
    "    \"\"\"\n",
    "    last_time = time.time()  # Store the previous observations in replay memory\n",
    "    D = load_object(\"D\")  # Load from file system\n",
    "\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] = 1  # 0 => Do Nothing ; 1 => Jump\n",
    "\n",
    "    # Get next step after performing the action\n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing)\n",
    "\n",
    "    # Stack 4 images to create a placeholder input\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "\n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  # 1*20*40*4\n",
    "\n",
    "    initial_state = s_t\n",
    "\n",
    "    if observe:  # We keep observing, never train\n",
    "        OBSERVE = 99999\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print(\"Loading weights to the CNN\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        #adam = Adam(lr=LEARNING_RATE)\n",
    "        nadam = Nadam(lr=LEARNING_RATE)\n",
    "        model.compile(loss=\"mse\", optimizer=nadam)\n",
    "        print(\"Loading weights Successful\")\n",
    "\n",
    "    else:  # We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = load_object(\"epsilon\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        #adam = Adam(lr=LEARNING_RATE)\n",
    "        nadam = Nadam(lr=LEARNING_RATE)\n",
    "        model.compile(loss=\"mse\", optimizer=nadam)\n",
    "\n",
    "    # Resume from the previous time step stored in the file system\n",
    "    t = load_object(\"time\")\n",
    "\n",
    "    while True:  # Endless running\n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0  # Reward at 4\n",
    "        a_t = np.zeros([ACTIONS])  # Actions at t\n",
    "\n",
    "        # Choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0:  # Parameter to skip frames for actions\n",
    "            if random.random() <= epsilon:  # Randomly explore an action\n",
    "                print(\"---------Random Action---------\")\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[action_index] = 1\n",
    "            else:  # Predict the output\n",
    "                # Input a stack of 4 images, get the prediction\n",
    "                q = model.predict(s_t)\n",
    "                max_Q = np.argmax(q)  # Choosing index with maximum \"q\" value\n",
    "                action_index = max_Q\n",
    "                a_t[action_index] = 1  # 0 => Do Nothing, 1 => Jump\n",
    "\n",
    "        # We reduce the epsilon (exploration parameter) gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON)/EXPLORE\n",
    "\n",
    "        # Run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "\n",
    "        # FPS of the game\n",
    "        print(\"FPS: {0}\".format(1/(time.time()-last_time)))\n",
    "        last_time = time.time()\n",
    "\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1)  # 1x20x40x1\n",
    "\n",
    "        # Append the new image to input stack and remove the first one\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "\n",
    "        # Store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        # Only train if done observing\n",
    "        if t > OBSERVE:\n",
    "            # Sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros(\n",
    "                (BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))  # 32x20x40x4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))\n",
    "\n",
    "            # Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]  # 4D stack of images\n",
    "                action_t = minibatch[i][1]  # This is the action index\n",
    "                reward_t = minibatch[i][2]  # Reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]  # Next State\n",
    "                # Wheather the agent died or survided due to the action\n",
    "                terminal = minibatch[i][4]\n",
    "\n",
    "                inputs[i:i+1] = state_t\n",
    "\n",
    "                targets[i] = model.predict(state_t)  # Predicted \"q\" value\n",
    "                # Predict \"q\" value for next step\n",
    "                Q_sa = model.predict(state_t1)\n",
    "\n",
    "                if terminal:\n",
    "                    # If terminated, only equal to reward\n",
    "                    targets[i, action_t] = reward_t\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "            q_values_df.loc[len(q_values_df)] = np.max(Q_sa)\n",
    "\n",
    "        # Reset game to initial frame if terminated\n",
    "        s_t = initial_state if terminal else s_t1\n",
    "        t += 1\n",
    "\n",
    "        # Save progress every 1000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Now we save model during training\")\n",
    "            game_state._game.pause()  # Pause game while saving to filesystem\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "            save_object(D, \"D\")  # Saving episodes\n",
    "            save_object(t, \"time\")  # Caching time steps\n",
    "            # Cache epsilon to avoide repeated randomness in actions\n",
    "            save_object(epsilon, \"epsilon\")\n",
    "            loss_df.to_csv(LOSS_FILE_PATH, index=False)\n",
    "            score_df.to_csv(SCORE_FILE_PATH, index=False)\n",
    "            actions_df.to_csv(ACTIONS_FILE_PATH, index=False)\n",
    "            q_values_df.to_csv(Q_VALUE_FILE_PATH, index=False)\n",
    "\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "\n",
    "            game_state._game.resume()\n",
    "\n",
    "        # Print Info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state, \"/ EPSILON\", epsilon, \"/ ACTION\",\n",
    "              action_index, \"/ REWARD\", r_t, \"/ Q_MAX \", np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to play the Game automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Function\n",
    "\n",
    "def playGame(observe=False):\n",
    "    game = Game()\n",
    "    dino = DinoAgent(game)\n",
    "    game_state = Game_State(dino, game)\n",
    "    model = buildModel()\n",
    "    try:\n",
    "        trainNetwork(model, game_state, observe=observe)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playGame(observe=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
